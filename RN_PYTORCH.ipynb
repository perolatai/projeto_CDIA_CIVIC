{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Bibliotecas\n",
    "\n",
    "import torch \n",
    "import pandas as pd \n",
    "import torch.nn as nn \n",
    "from torch.utils.data import random_split, DataLoader, TensorDataset \n",
    "import torch.nn.functional as F \n",
    "import numpy as np \n",
    "import torch.optim as optim \n",
    "from torch.optim import Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importação dos dados\n",
    "\n",
    "civic = pd.read_csv(r'civic') \n",
    "\n",
    "#print(civic.head()) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 3897 entries, 0 to 3896\n",
      "Data columns (total 39 columns):\n",
      " #   Column                        Non-Null Count  Dtype  \n",
      "---  ------                        --------------  -----  \n",
      " 0   gene                          3897 non-null   object \n",
      " 1   entrez_id                     3897 non-null   int64  \n",
      " 2   variant                       3897 non-null   object \n",
      " 3   disease                       3874 non-null   object \n",
      " 4   doid                          3865 non-null   float64\n",
      " 5   phenotypes                    508 non-null    object \n",
      " 6   drugs                         2449 non-null   object \n",
      " 7   drug_interaction_type         619 non-null    object \n",
      " 8   evidence_type                 3897 non-null   object \n",
      " 9   evidence_direction            3718 non-null   object \n",
      " 10  evidence_level                3897 non-null   object \n",
      " 11  clinical_significance         3675 non-null   object \n",
      " 12  evidence_statement            3897 non-null   object \n",
      " 13  citation_id                   3897 non-null   int64  \n",
      " 14  source_type                   3897 non-null   object \n",
      " 15  citation                      3897 non-null   object \n",
      " 16  rating                        3829 non-null   float64\n",
      " 17  evidence_status               3897 non-null   object \n",
      " 18  evidence_id                   3897 non-null   int64  \n",
      " 19  variant_id                    3897 non-null   int64  \n",
      " 20  gene_id                       3897 non-null   int64  \n",
      " 21  chromosome                    3462 non-null   object \n",
      " 22  start                         3461 non-null   float64\n",
      " 23  stop                          3461 non-null   float64\n",
      " 24  reference_bases               1897 non-null   object \n",
      " 25  variant_bases                 1842 non-null   object \n",
      " 26  representative_transcript     3431 non-null   object \n",
      " 27  ensembl_version               3422 non-null   float64\n",
      " 28  reference_build               3471 non-null   object \n",
      " 29  variant_summary               1387 non-null   object \n",
      " 30  variant_origin                3549 non-null   object \n",
      " 31  is_flagged                    3897 non-null   bool   \n",
      " 32  variant_types                 3265 non-null   object \n",
      " 33  hgvs_expressions              1943 non-null   object \n",
      " 34  civic_variant_evidence_score  3897 non-null   float64\n",
      " 35  allele_registry_id            1943 non-null   object \n",
      " 36  clinvar_ids                   2065 non-null   object \n",
      " 37  variant_aliases               2396 non-null   object \n",
      " 38  description                   3138 non-null   object \n",
      "dtypes: bool(1), float64(6), int64(5), object(27)\n",
      "memory usage: 1.1+ MB\n"
     ]
    }
   ],
   "source": [
    "civic.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Tranformando as colunas em números\n",
    "\n",
    "civic = civic[['gene', 'entrez_id', 'variant', 'disease', 'doid', 'phenotypes', 'drugs', 'drug_interaction_type', 'evidence_type', 'evidence_direction', 'evidence_level', 'clinical_significance', 'evidence_statement', 'citation_id', 'source_type', 'citation', 'rating', 'evidence_status', 'evidence_id', 'variant_id', 'gene_id', 'chromosome', 'start', 'stop', 'reference_bases', 'variant_bases', 'representative_transcript', 'ensembl_version', 'reference_build', 'variant_summary', 'variant_origin', 'is_flagged', 'variant_types', 'hgvs_expressions', 'civic_variant_evidence_score', 'allele_registry_id', 'clinvar_ids', 'variant_aliases', 'description']].apply(lambda x: pd.factorize(x)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input values are: \n",
      "   entrez_id  variant  disease  doid  phenotypes  drugs  \\\n",
      "0          0        0        0     0          -1     -1   \n",
      "1          1        1        1     1          -1     -1   \n",
      "2          2        2        2     2          -1     -1   \n",
      "3          2        2        2     2          -1     -1   \n",
      "4          0        0        3     3          -1     -1   \n",
      "\n",
      "   drug_interaction_type  evidence_type  evidence_direction  evidence_level  \\\n",
      "0                     -1              0                   0               0   \n",
      "1                     -1              0                   0               0   \n",
      "2                     -1              0                   0               0   \n",
      "3                     -1              0                   0               0   \n",
      "4                     -1              0                   0               0   \n",
      "\n",
      "   ...  ensembl_version  reference_build  variant_summary  variant_origin  \\\n",
      "0  ...                0                0                0               0   \n",
      "1  ...                0                0                1               0   \n",
      "2  ...                0                0                2               0   \n",
      "3  ...                0                0                2               0   \n",
      "4  ...                0                0                0               0   \n",
      "\n",
      "   is_flagged  variant_types  hgvs_expressions  civic_variant_evidence_score  \\\n",
      "0           0              0                 0                             0   \n",
      "1           0              1                 1                             1   \n",
      "2           0              1                -1                             2   \n",
      "3           0              1                -1                             2   \n",
      "4           0              0                 0                             0   \n",
      "\n",
      "   allele_registry_id  clinvar_ids  \n",
      "0                   0            0  \n",
      "1                   1            1  \n",
      "2                  -1            2  \n",
      "3                  -1            2  \n",
      "4                   0            0  \n",
      "\n",
      "[5 rows x 36 columns]\n",
      "Output value is: \n",
      "0    0\n",
      "1    1\n",
      "2    2\n",
      "3    2\n",
      "4    0\n",
      "Name: gene, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#Conversão em type\n",
    "\n",
    "label = {'gene': 0, 'disease': 1, 'clinical_significance': 2, }\n",
    "\n",
    "#Define as entradas e saidas do DataSet\n",
    "\n",
    "input = civic.iloc[:, 1:-2]         \n",
    "print('Input values are: ') \n",
    "print(input.head())   \n",
    "output = civic.loc[:, 'gene']         \n",
    "print('Output value is: ') \n",
    "print(output.head()) \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Input format:  torch.Size([3897, 36]) torch.float32\n",
      "Output format:  torch.Size([3897]) torch.int64\n"
     ]
    }
   ],
   "source": [
    "#Convertendo entradas e saidas em tensores e criando o DatasetTensor\n",
    "\n",
    "input = torch.Tensor(input.to_numpy())      \n",
    "print('\\nInput format: ', input.shape, input.dtype)      \n",
    "output = torch.tensor(output.to_numpy())         \n",
    "print('Output format: ', output.shape, output.dtype)   \n",
    "data = TensorDataset(input, output) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dividir para treinar, validar e testar conjuntos usando random_split\n",
    "\n",
    "train_batch_size = 10        \n",
    "number_rows = len(input)    \n",
    "test_split = int(number_rows*0.3)  \n",
    "validate_split = int(number_rows*0.2) \n",
    "train_split = number_rows - test_split - validate_split     \n",
    "train_set, validate_set, test_set = random_split( \n",
    "    data, [train_split, validate_split, test_split])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Criando Dataloader para ler dados em tamanhos de lote e colocá-los na memória\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size = train_batch_size, shuffle = True) \n",
    "validate_loader = DataLoader(validate_set, batch_size = 1) \n",
    "test_loader = DataLoader(test_set, batch_size = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define parâmetros do modelo\n",
    "\n",
    "input_size = list(input.shape)[1]    \n",
    "learning_rate = 0.01 \n",
    "output_size = len(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Definindo a rede neural\n",
    "\n",
    "class Network(nn.Module): \n",
    "   def __init__(self, input_size, output_size): \n",
    "       super(Network, self).__init__() \n",
    "        \n",
    "       self.layer1 = nn.Linear(input_size, 24) \n",
    "       self.layer2 = nn.Linear(24, 24) \n",
    "       self.layer3 = nn.Linear(24, output_size) \n",
    "\n",
    "\n",
    "   def forward(self, x): \n",
    "       x1 = F.relu(self.layer1(x)) \n",
    "       x2 = F.relu(self.layer2(x1)) \n",
    "       x3 = self.layer3(x2) \n",
    "       return x3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Instanciar o modelo\n",
    "\n",
    "model = Network(input_size, output_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The model will be running on cpu device\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Network(\n",
       "  (layer1): Linear(in_features=36, out_features=24, bias=True)\n",
       "  (layer2): Linear(in_features=24, out_features=24, bias=True)\n",
       "  (layer3): Linear(in_features=24, out_features=3, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Define o dispositivo de execução\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\") \n",
    "print(\"The model will be running on\", device, \"device\\n\") \n",
    "\n",
    "#Converte parâmetros do modelo e buffers para CPU ou Cuda\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função para salvar o modelo\n",
    "\n",
    "def saveModel(): \n",
    "    path = \"./NetModel.pth\" \n",
    "    torch.save(model.state_dict(), path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Define a função de perda com perda de entropia cruzada de classificação e um otimizador com Adam optimizer\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Função de treinamento\n",
    "\n",
    "def train(num_epochs): \n",
    "    best_accuracy = 0.0 \n",
    "     \n",
    "    print(\"Begin training...\") \n",
    "    for epoch in range(1, num_epochs+1): \n",
    "        running_train_loss = 0.0 \n",
    "        running_accuracy = 0.0 \n",
    "        running_vall_loss = 0.0 \n",
    "        total = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "Target 7 is out of bounds.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-18-19d428f7b3b8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mpredicted_outputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m     \u001b[0mtrain_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpredicted_outputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      9\u001b[0m     \u001b[0mtrain_loss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1499\u001b[0m                 \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1502\u001b[0m         \u001b[1;31m# Do not call functions when jit is used\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py\u001b[0m in \u001b[0;36mforward\u001b[1;34m(self, input, target)\u001b[0m\n\u001b[0;32m   1172\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1173\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minput\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1174\u001b[1;33m         return F.cross_entropy(input, target, weight=self.weight,\n\u001b[0m\u001b[0;32m   1175\u001b[0m                                \u001b[0mignore_index\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1176\u001b[0m                                label_smoothing=self.label_smoothing)\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[1;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[0;32m   3027\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0msize_average\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mreduce\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3028\u001b[0m         \u001b[0mreduction\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlegacy_get_string\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msize_average\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3029\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcross_entropy_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_Reduction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_enum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreduction\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlabel_smoothing\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3030\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3031\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: Target 7 is out of bounds."
     ]
    }
   ],
   "source": [
    "#Loop de treinamento\n",
    "\n",
    "for data in train_loader:\n",
    "    \n",
    "    inputs, outputs = data   \n",
    "    optimizer.zero_grad()             \n",
    "    predicted_outputs = model(inputs)    \n",
    "    train_loss = loss_fn(predicted_outputs, outputs)    \n",
    "    train_loss.backward()    \n",
    "    optimizer.step()        \n",
    "    running_train_loss +=train_loss.item()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
